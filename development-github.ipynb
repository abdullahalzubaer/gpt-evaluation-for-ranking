{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e25e491b-e474-4b88-9c89-fed150d0c232",
   "metadata": {},
   "source": [
    "# Functionality\n",
    "\n",
    "> Only development notebook where the goal is to save the ranking in a csv file only (analysis will be done on evaluation.ipynb notebook\"\n",
    "\n",
    "1. The user provides a prompt containing that ask the LM to rank for n sentences with the ideal sentence based on similarity.\n",
    "\n",
    "2. Save the response (as dataframe) from the model and extract the rank from the free text generated by the model and save it into the same dataframe as a new column\n",
    "\n",
    "3. Creating a column that has the correlation for each prompt with the gold rank (here you provide the gold rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "479a871e-d775-4865-ab89-71226177bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import ipywidgets as widgets\n",
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import ast\n",
    "\n",
    "# this is saved as environment variable in the system.\n",
    "openai.api_key = os.getenv(\"---------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b88cbf3-d5b1-490b-9277-33870b3212a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_rank_example = widgets.Dropdown(options=[\"[2, 3, 4, 1, 5]\"], value=\"[2, 3, 4, 1, 5]\", description='Gold Rank Example', layout={'height': 'auto', \"width\":\"auto\"})\n",
    "# display(gold_rank_example)\n",
    "\n",
    "insert_gold_rank_manually = widgets.Textarea(placeholder='Enter Gold Rank in the format of this [1,2,3,4,5]', value='', description='Gold Rank', rows=1, layout={'height': 'auto', \"width\":\"auto\"})\n",
    "display(insert_gold_rank_manually)\n",
    "\n",
    "\n",
    "prompt_identifier = widgets.Dropdown(options=[\"cot\", \"zero-shot\", \"one-shot-1\",\"one-shot-2\", \"one-shot-3\" , \"two-shot\"], value=None, description='Prompt Type Identifier', layout={'height': 'auto', \"width\":\"auto\"})\n",
    "display(prompt_identifier)\n",
    "\n",
    "text_input = widgets.Textarea(placeholder='Prompt', value='', description='Enter text:', rows=5, layout={'height': 'auto', \"width\":\"auto\"})\n",
    "display(text_input)\n",
    "\n",
    "\n",
    "# Creating widgets for the hyperparameters\n",
    "model_input = widgets.Dropdown(options=[\"gpt-3.5-turbo\", \"gpt-4\", \"text-davinci-003\"], value='gpt-3.5-turbo', description='model:', layout={'height': 'auto', \"width\":\"auto\"})\n",
    "display(model_input)\n",
    "temp_input = widgets.FloatSlider(value=0, min=0, max=1, step=0.05, description='temperature:', layout={'height': 'auto', \"width\":\"auto\"})\n",
    "display(temp_input)\n",
    "max_tokens_input = widgets.IntSlider(value=500, min=1, max=4000, step=1, description='max_tokens:', layout={'height': 'auto', \"width\":\"auto\"})\n",
    "display(max_tokens_input)\n",
    "top_p_input = widgets.FloatSlider(value=1.0, min=0, max=1, step=0.05, description='top_p:', layout={'height': 'auto', \"width\":\"auto\"})\n",
    "display(top_p_input)\n",
    "# freq_penalty_input = widgets.FloatSlider(value=0.0, min=0, max=1, step=0.05, description='frequency_penalty:', layout={'height': 'auto', \"width\":\"auto\"})\n",
    "# display(freq_penalty_input)\n",
    "# presence_penalty_input = widgets.FloatSlider(value=0.0, min=0, max=1, step=0.05, description='presence_penalty:', layout={'height': 'auto', \"width\":\"auto\"})\n",
    "# display(presence_penalty_input)\n",
    "# Creating the csv file if it does not exist and adding the new column \"Hyperparameters used\"\n",
    "\n",
    "\n",
    "if not os.path.exists('response_eco.csv'):\n",
    "    with open(\"response_eco.csv\", \"a\", newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Prompt With Task\",\n",
    "                         \"Gold Rank\",\n",
    "                         \"Prompt Type\",\n",
    "                         \"Complete Response from Model\",\n",
    "                         \"Output Only From Model\",\n",
    "                         \"Hyperparameters used\",\n",
    "                         \"Date and Time\",\n",
    "                        \"Model\"])\n",
    "\n",
    "# Creating the function that will be called on button click\n",
    "\n",
    "def on_submit(b):\n",
    "    entered_text = text_input.value\n",
    "    \n",
    "    if model_input.value == \"gpt-4\" or model_input.value == \"gpt-3.5-turbo\":\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model_input.value,\n",
    "            messages=[{\"role\": \"user\", \"content\": entered_text}],\n",
    "            # prompt=entered_text,\n",
    "            temperature=temp_input.value,\n",
    "            max_tokens=max_tokens_input.value,\n",
    "            top_p=top_p_input.value,\n",
    "            # frequency_penalty=freq_penalty_input.value,\n",
    "            # presence_penalty=presence_penalty_input.value,\n",
    "        )\n",
    "        \n",
    "        print(\"--------------------------------------------------------------------------------\")\n",
    "        print(\"Input: {}\\nOutput: \\n \\n{}\\n\".format(entered_text, response['choices'][0]['message']['content']))\n",
    "\n",
    "    elif model_input.value == \"text-davinci-003\":\n",
    "        \n",
    "        response = openai.Completion.create(\n",
    "            model=model_input.value,\n",
    "            prompt=entered_text,\n",
    "            temperature=temp_input.value,\n",
    "            max_tokens=max_tokens_input.value,\n",
    "            top_p=top_p_input.value,\n",
    "            # frequency_penalty=freq_penalty_input.value,\n",
    "            # presence_penalty=presence_penalty_input.value,\n",
    "        )\n",
    "        print(\"--------------------------------------------------------------------------------\")\n",
    "        print(\"Input: {}\\nOutput: \\n \\n{}\\n\".format(entered_text, response['choices'][0][\"text\"]))\n",
    "        \n",
    "    now = datetime.datetime.now()\n",
    "    # Storing the hyperparameters used in the response\n",
    "    params = {\n",
    "        'model': model_input.value, \n",
    "        'temperature': temp_input.value, \n",
    "        'max_tokens': max_tokens_input.value, \n",
    "        'top_p': top_p_input.value, \n",
    "        # 'frequency_penalty': freq_penalty_input.value, \n",
    "        # 'presence_penalty': presence_penalty_input.value\n",
    "    }\n",
    "    \n",
    "    \n",
    "    if model_input.value == \"gpt-4\" or model_input.value == \"gpt-3.5-turbo\":\n",
    "        print(\"I am in gpt-3.5-turbo\")\n",
    "        data = [\n",
    "            entered_text,\n",
    "            insert_gold_rank_manually.value,\n",
    "            prompt_identifier.value,\n",
    "            response,\n",
    "            response['choices'][0]['message']['content'],\n",
    "            params,\n",
    "            now.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            model_input.value\n",
    "               ]\n",
    "        \n",
    "    elif model_input.value == \"text-davinci-003\":\n",
    "        data = [\n",
    "            entered_text,\n",
    "            insert_gold_rank_manually.value,\n",
    "            prompt_identifier.value,\n",
    "            response,\n",
    "            response['choices'][0][\"text\"], # for text-davinci-003\n",
    "            # response['choices'][0]['message']['content'],\n",
    "            params,\n",
    "            now.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            model_input.value\n",
    "               ]\n",
    "        \n",
    "    # Open the CSV file for writing\n",
    "    with open(\"response_eco.csv\", \"a\", newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the data rows\n",
    "        writer.writerow(data)\n",
    "\n",
    "\n",
    "submit_button = widgets.Button(description=\"Submit\")\n",
    "submit_button.on_click(on_submit)\n",
    "display(submit_button)\n",
    "\n",
    "# gold rank\n",
    "# [2, 3, 4, 1, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3587da-8a3f-48f8-af86-637527140a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"response_eco.csv\", encoding='cp1252')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866f60cf-509c-44fb-a319-3b4a4fe6ecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[-1]['Output Only From Model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bf1e16-be24-4817-8228-2abf06279385",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functionality:\n",
    "\n",
    "Extract the rank from the model's output (which is saved in the csv file)\n",
    "and create a new column containing (in the same df) the ranks in a list.\n",
    "'''\n",
    "\n",
    "# Define a function to extract the numbers associated with Rank\n",
    "def extract_ranks(text):\n",
    "    return [int(x) for x in re.findall(r'SAMPLE_TEXT_\\d+: Rank (\\d+)', text)]\n",
    "\n",
    "# Apply the function to the column of the dataframe\n",
    "df['model ranks'] = df['Output Only From Model'].apply(extract_ranks)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee235db-b5fe-4137-b1b0-734be7039d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functionality: converting the gold rank that is in string format\n",
    "now to a list of integer\n",
    "\n",
    "(If I have pre-processed it while saving the gold-rank then I would not\n",
    "have to do it. With string you cannot calcualte spearman rank correlation)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def str_to_list(str_list):\n",
    "    # Remove the brackets from the string\n",
    "    input_str = str_list[1:-1]\n",
    "\n",
    "    # Split the string at the commas\n",
    "    input_list = input_str.split(\",\")\n",
    "\n",
    "    # Convert each element to an integer\n",
    "    input_list = [int(x) for x in input_list]\n",
    "\n",
    "    return input_list\n",
    "\n",
    "df['Gold Rank'] = df['Gold Rank'].apply(str_to_list)\n",
    "type(df.iloc[-1]['Gold Rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a02429-d96c-4010-9e2c-e783760f1b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "'''\n",
    "For all the rank we got and create a another column with it\n",
    "'''\n",
    "def calc_spearman_rank_corr(row):\n",
    "    gold_rank = row['Gold Rank']\n",
    "    model_ranks = row['model ranks']\n",
    "    corr, pvalue = spearmanr(gold_rank, model_ranks)\n",
    "    return corr\n",
    "\n",
    "df['spearman_rank_correlation'] = df.apply(calc_spearman_rank_corr, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2dd1e66b-0747-42a1-8077-9c79ba7e64cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functionarlity:\n",
    "\n",
    "Saving the final dataframe with all the ranks extracted and the spearman\n",
    "rank correlation FINALLY!\n",
    "'''\n",
    "\n",
    "df.to_csv('complete_response_with_spearman.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "9f9a1ad2-e67a-4a46-8b18-84161da95827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++++++++++++++++\n",
      "Number of Samples: 18\n",
      "Average Correlation: 0.2722\n",
      "Variance: 0.2657\n",
      "StandardDeviation: 0.5154\n",
      "+++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "sample_length = len(df)\n",
    "average_correlation = statistics.mean(df['spearman_rank_correlation'])\n",
    "variance = statistics.variance(df['spearman_rank_correlation'])\n",
    "standard_deviation = statistics.stdev(df['spearman_rank_correlation'])\n",
    "                                      \n",
    "print(\"+++++++++++++++++++++++++++++\")\n",
    "print(f\"Number of Samples: {sample_length}\\nAverage Correlation: {average_correlation:.4f}\\nVariance: {variance:.4f}\\nStandardDeviation: {standard_deviation:.4f}\")\n",
    "print(\"+++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0888dc-15d0-4a0f-9675-db84e0a41243",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extra Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b0e9ab-d9cf-4c33-937f-9ce5a5780e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"USE THIS IF THE OUTPUT IS LIKE SHOWN BELOW TO EXTRACT THE RANKS: ACTUALLY YOU DO NOT\n",
    "need this if you write this in the zero shot prompt\n",
    "=========================================\n",
    "Provide the rank in the following format\n",
    "\n",
    "SAMPLE_TEXT_X: Rank X\n",
    "==========================================\n",
    "\"\"\"\n",
    "\n",
    "# define the function to apply\n",
    "def extract_numbers_from_template_one_shot_1_to_cot(text):\n",
    "    '''\n",
    "    This is exactly for \"zero-shot\" template because the output of the model is different like this\n",
    "    \n",
    "    1. SAMPLE_TEXT_4 \n",
    "    2. SAMPLE_TEXT_1 \n",
    "    3. SAMPLE_TEXT_6 \n",
    "    4. SAMPLE_TEXT_2 \n",
    "    5. SAMPLE_TEXT_5 \n",
    "    6. SAMPLE_TEXT_3\n",
    "    '''\n",
    "    pattern = r\"(?:Sample Text (\\d+)|SAMPLE_TEXT_(\\d+))$\"\n",
    "    numbers = re.findall(pattern, text)\n",
    "\n",
    "    # Convert strings to integers\n",
    "    numbers = [int(num1 or num2) for num1, num2 in re.findall(pattern, text, re.MULTILINE)]\n",
    "\n",
    "\n",
    "    return numbers\n",
    "\n",
    "\n",
    "df['model ranks'] = df.apply(lambda x: extract_numbers_from_template_one_shot_1_to_cot(x['Output']) if x['Prompt']=='zero-shot' else x['model ranks'], axis=1)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
