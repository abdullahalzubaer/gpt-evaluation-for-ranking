{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e25e491b-e474-4b88-9c89-fed150d0c232",
   "metadata": {},
   "source": [
    "# Functionality\n",
    "\n",
    "> Only development notebook where the goal is to save the ranking in a csv file only (analysis will be done on evaluation.ipynb notebook\"\n",
    "\n",
    "1. The user provides a prompt containing that ask the LM to rank for n sentences with the ideal sentence based on similarity.\n",
    "\n",
    "2. Save the response (as dataframe) from the model and extract the rank from the free text generated by the model and save it into the same dataframe as a new column\n",
    "\n",
    "3. Creating a column that has the correlation for each prompt with the gold rank (here you provide the gold rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479a871e-d775-4865-ab89-71226177bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import ipywidgets as widgets\n",
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import ast\n",
    "\n",
    "# this is saved as environment variable in the system.\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY_MICHAEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9be735-ef36-43de-9fec-0def1402dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_rank_example = widgets.Dropdown(options=[\"[2, 3, 4, 1, 5]\"], value=\"[2, 3, 4, 1, 5]\", description='Gold Rank Example', layout={'height': 'auto', \"width\":\"auto\"})\n",
    "# display(gold_rank_example)\n",
    "\n",
    "insert_gold_rank_manually = widgets.Textarea(placeholder='Enter Gold Rank in the format of this [1,2,3,4,5]', value='', description='Gold Rank', rows=1, layout={'height': 'auto', \"width\":\"auto\"})\n",
    "display(insert_gold_rank_manually)\n",
    "\n",
    "\n",
    "prompt_identifier = widgets.Dropdown(options=[\"cot\", \"zero-shot\", \"one-shot-1\",\"one-shot-2\", \"one-shot-3\" , \"two-shot\"], value=None, description='Prompt Type Identifier', layout={'height': 'auto', \"width\":\"auto\"})\n",
    "display(prompt_identifier)\n",
    "\n",
    "text_input = widgets.Textarea(placeholder='Prompt', value='', description='Enter text:', rows=5, layout={'height': 'auto', \"width\":\"auto\"})\n",
    "display(text_input)\n",
    "\n",
    "\n",
    "# Creating widgets for the hyperparameters\n",
    "model_input = widgets.Dropdown(options=[\"gpt-3.5-turbo\", \"gpt-4\", \"text-davinci-003\"], value='gpt-3.5-turbo', description='model:', layout={'height': 'auto', \"width\":\"auto\"})\n",
    "display(model_input)\n",
    "temp_input = widgets.FloatSlider(value=0, min=0, max=1, step=0.05, description='temperature:', layout={'height': 'auto', \"width\":\"auto\"})\n",
    "display(temp_input)\n",
    "max_tokens_input = widgets.IntSlider(value=500, min=1, max=4000, step=1, description='max_tokens:', layout={'height': 'auto', \"width\":\"auto\"})\n",
    "display(max_tokens_input)\n",
    "top_p_input = widgets.FloatSlider(value=1.0, min=0, max=1, step=0.05, description='top_p:', layout={'height': 'auto', \"width\":\"auto\"})\n",
    "display(top_p_input)\n",
    "# freq_penalty_input = widgets.FloatSlider(value=0.0, min=0, max=1, step=0.05, description='frequency_penalty:', layout={'height': 'auto', \"width\":\"auto\"})\n",
    "# display(freq_penalty_input)\n",
    "# presence_penalty_input = widgets.FloatSlider(value=0.0, min=0, max=1, step=0.05, description='presence_penalty:', layout={'height': 'auto', \"width\":\"auto\"})\n",
    "# display(presence_penalty_input)\n",
    "# Creating the csv file if it does not exist and adding the new column \"Hyperparameters used\"\n",
    "\n",
    "\n",
    "if not os.path.exists('response_eco.csv'):\n",
    "    with open(\"response_eco.csv\", \"a\", newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Prompt With Task\",\n",
    "                         \"Gold Rank\",\n",
    "                         \"Prompt Type\",\n",
    "                         \"Complete Response from Model\",\n",
    "                         \"Output Only From Model\",\n",
    "                         \"Hyperparameters used\",\n",
    "                         \"Date and Time\",\n",
    "                        \"Model\"])\n",
    "\n",
    "# Creating the function that will be called on button click\n",
    "\n",
    "def on_submit(b):\n",
    "    entered_text = text_input.value\n",
    "    \n",
    "    if model_input.value == \"gpt-4\" or model_input.value == \"gpt-3.5-turbo\":\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model_input.value,\n",
    "            messages=[{\"role\": \"user\", \"content\": entered_text}],\n",
    "            # prompt=entered_text,\n",
    "            temperature=temp_input.value,\n",
    "            max_tokens=max_tokens_input.value,\n",
    "            top_p=top_p_input.value,\n",
    "            # frequency_penalty=freq_penalty_input.value,\n",
    "            # presence_penalty=presence_penalty_input.value,\n",
    "        )\n",
    "        \n",
    "        print(\"--------------------------------------------------------------------------------\")\n",
    "        print(\"Input: {}\\nOutput: \\n \\n{}\\n\".format(entered_text, response['choices'][0]['message']['content']))\n",
    "\n",
    "    elif model_input.value == \"text-davinci-003\":\n",
    "        \n",
    "        response = openai.Completion.create(\n",
    "            model=model_input.value,\n",
    "            prompt=entered_text,\n",
    "            temperature=temp_input.value,\n",
    "            max_tokens=max_tokens_input.value,\n",
    "            top_p=top_p_input.value,\n",
    "            # frequency_penalty=freq_penalty_input.value,\n",
    "            # presence_penalty=presence_penalty_input.value,\n",
    "        )\n",
    "        print(\"--------------------------------------------------------------------------------\")\n",
    "        print(\"Input: {}\\nOutput: \\n \\n{}\\n\".format(entered_text, response['choices'][0][\"text\"]))\n",
    "        \n",
    "    now = datetime.datetime.now()\n",
    "    # Storing the hyperparameters used in the response\n",
    "    params = {\n",
    "        'model': model_input.value, \n",
    "        'temperature': temp_input.value, \n",
    "        'max_tokens': max_tokens_input.value, \n",
    "        'top_p': top_p_input.value, \n",
    "        # 'frequency_penalty': freq_penalty_input.value, \n",
    "        # 'presence_penalty': presence_penalty_input.value\n",
    "    }\n",
    "    \n",
    "    \n",
    "    if model_input.value == \"gpt-4\" or model_input.value == \"gpt-3.5-turbo\":\n",
    "        print(\"I am in gpt-3.5-turbo\")\n",
    "        data = [\n",
    "            entered_text,\n",
    "            insert_gold_rank_manually.value,\n",
    "            prompt_identifier.value,\n",
    "            response,\n",
    "            response['choices'][0]['message']['content'],\n",
    "            params,\n",
    "            now.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            model_input.value\n",
    "               ]\n",
    "        \n",
    "    elif model_input.value == \"text-davinci-003\":\n",
    "        data = [\n",
    "            entered_text,\n",
    "            insert_gold_rank_manually.value,\n",
    "            prompt_identifier.value,\n",
    "            response,\n",
    "            response['choices'][0][\"text\"], # for text-davinci-003\n",
    "            # response['choices'][0]['message']['content'],\n",
    "            params,\n",
    "            now.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            model_input.value\n",
    "               ]\n",
    "        \n",
    "    # Open the CSV file for writing\n",
    "    with open(\"response_eco.csv\", \"a\", newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the data rows\n",
    "        writer.writerow(data)\n",
    "\n",
    "\n",
    "submit_button = widgets.Button(description=\"Submit\")\n",
    "submit_button.on_click(on_submit)\n",
    "display(submit_button)\n",
    "\n",
    "# gold rank\n",
    "# [2, 3, 4, 1, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cffcdec-736a-4fbb-a740-07db592ef62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "response_eco has the complete response and everything else that we have got by prompting\n",
    "the LLM\n",
    "'''\n",
    "df = pd.read_csv(\"response_eco.csv\", encoding='cp1252')\n",
    "print(df.iloc[11]['Prompt With Task'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adbe1a2-bdd6-42d3-82e0-81707c3d9e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[-1]['Output Only From Model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba23dc9-3789-4eb4-8c43-9283dbaba5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functionality:\n",
    "\n",
    "Extract the rank from the model's output (which is saved in the csv file)\n",
    "and create a new column containing (in the same df) the ranks in a list.\n",
    "'''\n",
    "\n",
    "# Define a function to extract the numbers associated with Rank\n",
    "def extract_ranks(text):\n",
    "    return [int(x) for x in re.findall(r'SAMPLE_TEXT_\\d+: Rank (\\d+)', text)]\n",
    "\n",
    "# Apply the function to the column of the dataframe\n",
    "df['model ranks'] = df['Output Only From Model'].apply(extract_ranks)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42ee7e4-f919-47e2-b805-95ffc6ac2371",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functionality: converting the gold rank that is in string format\n",
    "now to a list of integer\n",
    "\n",
    "(If I have pre-processed it while saving the gold-rank then I would not\n",
    "have to do it. With string you cannot calcualte spearman rank correlation)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def str_to_list(str_list):\n",
    "    # Remove the brackets from the string\n",
    "    input_str = str_list[1:-1]\n",
    "\n",
    "    # Split the string at the commas\n",
    "    input_list = input_str.split(\",\")\n",
    "\n",
    "    # Convert each element to an integer\n",
    "    input_list = [int(x) for x in input_list]\n",
    "\n",
    "    return input_list\n",
    "\n",
    "df['Gold Rank'] = df['Gold Rank'].apply(str_to_list)\n",
    "type(df.iloc[-1]['Gold Rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cd54f7-df53-49a4-9411-1039fc6f9d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "'''\n",
    "For all the rank we got and create a another column with it\n",
    "'''\n",
    "def calc_spearman_rank_corr(row):\n",
    "    gold_rank = row['Gold Rank']\n",
    "    model_ranks = row['model ranks']\n",
    "    corr, pvalue = spearmanr(gold_rank, model_ranks)\n",
    "    print(corr)\n",
    "    return corr\n",
    "\n",
    "df['spearman_rank_correlation'] = df.apply(calc_spearman_rank_corr, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd1e66b-0747-42a1-8077-9c79ba7e64cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functionarlity:\n",
    "\n",
    "Saving the final dataframe with all the ranks extracted and the spearman\n",
    "rank correlation FINALLY!\n",
    "'''\n",
    "\n",
    "df.to_csv('complete_response_with_spearman.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823ef411-fb1a-460a-88b0-4450b7b820ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_length = len(df)\n",
    "average_correlation = statistics.mean(df['spearman_rank_correlation'])\n",
    "variance = statistics.variance(df['spearman_rank_correlation'])\n",
    "standard_deviation = statistics.stdev(df['spearman_rank_correlation'])\n",
    "                                      \n",
    "print(\"+++++++++++++++++++++++++++++\")\n",
    "print(f\"Number of Samples: {sample_length}\\nAverage Correlation: {average_correlation:.4f}\\nVariance: {variance:.4f}\\nStandardDeviation: {standard_deviation:.4f}\")\n",
    "print(\"+++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acec5a4-abc0-4ca0-ad65-ec6721b39cb7",
   "metadata": {},
   "source": [
    "# Now using Local Model for ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cbadd0-60aa-4579-b2ee-29c97ccc5443",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a simple application for sentence embeddings: semantic search\n",
    "\n",
    "We have a corpus with various sentences. Then, for a given query sentence,\n",
    "we want to find the most similar sentence in this corpus.\n",
    "\n",
    "This script outputs for various queries the top 5 most similar sentences in the corpus.\n",
    "which can be easily canged by chanigng the top_k parameter\n",
    "\"\"\"\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "embedder = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "\n",
    "# Corpus with example sentences\n",
    "corpus = ['Löhne werden regelmäßig erhöht. Jedoch werden die Löhne der Jobs, die eher keine Zukunft mehr haben niedriger ausfallen und wann will sie nicht mehr erhöhen. Dies wird aber von den Menschen als unfair bezeichnet, darum entstehen Streiks. Durch eine Inflation erhöhen sich die Lebenshaltungskosten, die Löhne bleiben aber bei diesen Jobs gleich. Hier führt es aber nicht zu Streiks, da es eine versteckte Jobentwertung ist. Somit sind die Löhne flexibler.',\n",
    "          'Löhne sollten sich gemäß der Produktivität verändern. Eine positive Lohnveränderung kann aber nur bei einer positiven Produktionslücke stattfinden. Eine positive Produktionslücke bewirkt eine ansteigende Produktion. Maschinen verschleißen dann schneller und Produzenten müssen die Preise erhöhen. Dadurch steigt auch der Lohn, da der Preis einer Arbeitsstunde ebenfalls nach oben geht. Mit steigender Inflation können Löhne also flexibilisiert werden.',\n",
    "          'insbesondere für kriselnde branchen und unternehmen birgt inflation finanzielle vorteile. denn durch diese sind („inflationsinfizierte“) lohnsenkungen möglich ohne das diese einen ähnlichen aufschrei der arbeitnehmer:innen zur folge hätte als eine herkömliche lohnsenkung. folglich sind mehr finanzielle mittel zur verfügung. des weiteren haben alle kreditnehmer:innen den vorteil, dass dieses relativ gesehen günstiger zurückzuzahlen ist.',\n",
    "          'Aus einer Inflation können kurzfr. Steigerungen der Produktivität resultieren. Dies hat positive Auswirkungen auf die Löhne in den Branchen mit gesteigerter Produktivität. In Branchen mit verminderter Produktivität sollten eigentlich Lohnsenkungen das Resultat sein. Da diese als ungerecht empfunden werden bleiben diese aber oft aus. Somit führt die Inflation zu einer Senkung der Reallöhne bei gleich bleibenden nom. Löhnen und hat somit einen pos. Einfluss auf die Flexibilität der Löhne.',\n",
    "          'Die Inflation beeinflusst die Löhne durch ihre Verzögerte Anpassung. Nehmen wir an die Löhne wären am Tag x festgelegt. Die Inflation steigt Zwischen X und Y um 10%. Da die Löhne Kollektiv verhandelt werden, kann eine Anpassung erst viel später folgen. Daraus Folgt dass die tatsächlichen Löhne gleich bleiben, das Unternehmen kann aber die Preise ihrer Produkte schneller anpassen. Ergo eine Gewinnsteigerung für Unternehmen.',\n",
    "          ]\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdb9497-aaf6-4126-8aa8-470159e6e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3588ae-8d08-4e3d-9dbe-8453b6f0cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Query sentences:\n",
    "queries = ['Löhne sollten mit erhöhter Produktivität eines Unternehmens steigen. Sinkt in einem Unternehmen die Produktivität, beispielsweise in sterbenden Branchen, so sollten in diesem die Löhne dementsprechend sinken. Eine solche Lohnsenkung wird jedoch von ArbeitnehmerInnen als unfair wahrgenommen und kann zu einer verringerten Arbeitsmoral und möglicherweise sogar zu Streiks führen. Bei Vorhandensein von Inflation kann eine Lohnsenkung dadurch erfolgen, dass Löhne nominal konstant bleiben und durch die Preissteigerung real entwertet werden. Dies wird oft als weniger unfair empfunden. So kann Inflation Unternehmen mehr Flexibilität geben, um eine unvermeidliche Lohnsenkung durchzuführen.']\n",
    "\n",
    "\n",
    "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "top_k = min(5, len(corpus))\n",
    "for query in queries:\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "    # print(query_embedding)\n",
    "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
    "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    print(cos_scores)\n",
    "    top_results = torch.topk(cos_scores, k=top_k, sorted=True)\n",
    "\n",
    "    print(\"\\n\\n======================\\n\\n\")\n",
    "    print(\"Query:\", query)\n",
    "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\n",
    "        print(corpus[idx], \"(Score: {:.4f})\".format(score))\n",
    "        print(\"\\n\")\n",
    "\n",
    "    \"\"\"\n",
    "    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk\n",
    "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)\n",
    "    hits = hits[0]      #Get the hits for the first query\n",
    "    for hit in hits:\n",
    "        print(corpus[hit['corpus_id']], \"(Score: {:.4f})\".format(hit['score']))\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdb65b5-a085-458a-b995-ae7ec9151605",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "print(cos_scores)\n",
    "top_results = torch.topk(cos_scores, k=5, sorted=True)\n",
    "top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d8ca73-52cd-422e-91ce-dbebb9a6a109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Rank by local model\n",
    "[2,1,5,3,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd52285-f527-48c3-a7b4-677527fba1b0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Explanation of how I arrived to the rank below from top_results.indices\n",
    "\n",
    "indices=tensor([1, 0, 3, 4, 2])) <- list_K\n",
    "\n",
    "The number that you can see in the indices are corresponding to the index of the corpus which has the 5 sentences.\n",
    "Therefore, we see 4 in the above list, which means this is the index 4 of corpus list. And on index 4 we have this sentence\n",
    "\"Die Inflation beeinflusst die Löhne durch ihre Verzögerte Anpassung. Nehmen wir an die Löhne wären am Tag x festgelegt. Die Inflation steigt Zwischen X und Y um 10%. Da die Löhne Kollektiv verhandelt werden, kann eine Anpassung erst viel später folgen. Daraus Folgt dass die tatsächlichen Löhne gleich bleiben, das Unternehmen kann aber die Preise ihrer Produkte schneller anpassen. Ergo eine Gewinnsteigerung für Unternehmen.\"\n",
    "\n",
    "This senence is in the index of 3 in the above list (list_K), which means the rank is 4 (I am assuming rank start from 1)\n",
    "\n",
    "Another example.\n",
    "\n",
    "We see 2 in the above list (list_K), wehich means this is the index 2 of the corpus list, and on index 2 we have this sentence:\n",
    "\"insbesondere für kriselnde branchen und unternehmen birgt inflation finanzielle vorteile. denn durch diese sind („inflationsinfizierte“) lohnsenkungen möglich ohne das diese einen ähnlichen aufschrei der arbeitnehmer:innen zur folge hätte als eine herkömliche lohnsenkung. folglich sind mehr finanzielle mittel zur verfügung. des weiteren haben alle kreditnehmer:innen den vorteil, dass dieses relativ gesehen günstiger zurückzuzahlen ist.'\"\n",
    "\n",
    "This sentence is in the index  of 4 in the above list (list_K) whihc means it is ranked as 5.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SAMPLE_TEXT_0 = RANK 2\n",
    "\n",
    "SAMPLE_TEXT_1 = RANK 1\n",
    "\n",
    "SAMPLE_TEXT_2 = RANK 5\n",
    "\n",
    "SAMPLE_TEXT_3 = RANK 3\n",
    "\n",
    "SAMPLE_TEXT_4 = RANK 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fff076f-55d7-4100-9c22-84d28f65a5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging this new rank to the complete dataframe that has the rank from the three model\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "Reading the complete response that has the spearman correlation also that I have\n",
    "done previously. In here we do not have the info about the local model's rank\n",
    "'''\n",
    "df_temp = pd.read_csv(\"complete_response_with_spearman.csv\")\n",
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1606b1e0-eb28-4829-8e02-a4d31e16d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "'''\n",
    "Calculating the sperman rank correlation with the ranking provided by\n",
    "sentence transformer with the gold rank\n",
    "'''\n",
    "\n",
    "corr, pvalue = spearmanr([2, 3, 4, 1, 5], [2,1,5,3,4])\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39d0022-8f66-4e92-b60c-2f380f0f3c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "# corr, pvalue = spearmanr([2, 3, 4, 1, 5], [2,4,3,1,5])\n",
    "# corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e435bfc5-92da-49d3-b9ad-726eb79c4b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your existing DataFrame is called \"df_temp\"  \n",
    "\n",
    "'''\n",
    "Creating this dictionary that has the new information for the local model\n",
    "to append it to the previous dataframe that has all the information except the local model's rank\n",
    "'''\n",
    "\n",
    "new_row = {\"Model\": \"distiluse-base-multilingual-cased-v1\",\n",
    "           \"model ranks\": [2, 1, 5, 3, 4],\n",
    "           \"spearman_rank_correlation\": 0.50,\n",
    "           \"Prompt Type\": \"zero-shot\"}\n",
    "# specifying inwhich location we should add the new item\n",
    "df_temp.loc[len(df_temp)] = new_row\n",
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05a49db-afb9-4b88-96f2-763c9a595c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functionarlity:\n",
    "\n",
    "Saving the final dataframe with all the ranks extracted and the spearman\n",
    "rank correlation which also include the information of the local models rank\n",
    "'''\n",
    "\n",
    "df_temp.to_csv('complete_response_with_spearman_with_localmodel_also.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6428fedf-b37f-4d97-8edb-65958377dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0888dc-15d0-4a0f-9675-db84e0a41243",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extra Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b0e9ab-d9cf-4c33-937f-9ce5a5780e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"USE THIS IF THE OUTPUT IS LIKE SHOWN BELOW TO EXTRACT THE RANKS: ACTUALLY YOU DO NOT\n",
    "need this if you write this in the zero shot prompt\n",
    "=========================================\n",
    "Provide the rank in the following format\n",
    "\n",
    "SAMPLE_TEXT_X: Rank X\n",
    "==========================================\n",
    "\"\"\"\n",
    "\n",
    "# define the function to apply\n",
    "def extract_numbers_from_template_one_shot_1_to_cot(text):\n",
    "    '''\n",
    "    This is exactly for \"zero-shot\" template because the output of the model is different like this\n",
    "    \n",
    "    1. SAMPLE_TEXT_4 \n",
    "    2. SAMPLE_TEXT_1 \n",
    "    3. SAMPLE_TEXT_6 \n",
    "    4. SAMPLE_TEXT_2 \n",
    "    5. SAMPLE_TEXT_5 \n",
    "    6. SAMPLE_TEXT_3\n",
    "    '''\n",
    "    pattern = r\"(?:Sample Text (\\d+)|SAMPLE_TEXT_(\\d+))$\"\n",
    "    numbers = re.findall(pattern, text)\n",
    "\n",
    "    # Convert strings to integers\n",
    "    numbers = [int(num1 or num2) for num1, num2 in re.findall(pattern, text, re.MULTILINE)]\n",
    "\n",
    "\n",
    "    return numbers\n",
    "\n",
    "\n",
    "df['model ranks'] = df.apply(lambda x: extract_numbers_from_template_one_shot_1_to_cot(x['Output']) if x['Prompt']=='zero-shot' else x['model ranks'], axis=1)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
